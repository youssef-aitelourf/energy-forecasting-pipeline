# ğŸ”‹ Energy Forecasting Pipeline: Advanced ML for Consumption Prediction

<div align="center">

[![Python 3.8+](https://img.shields.io/badge/Python-3.8%2B-blue)](https://www.python.org/downloads/)
[![License: MIT](https://img.shields.io/badge/License-MIT-green.svg)](LICENSE)
[![Status: Production Ready](https://img.shields.io/badge/Status-Production%20Ready-success)]()

*A professional machine learning pipeline demonstrating production-grade ML engineering practices*

</div>

---

## ğŸ“‹ Overview

This project showcases a **complete, production-ready ML pipeline** for energy consumption forecasting. It demonstrates:

- âœ… **Professional code structure** with modularized components
- âœ… **End-to-end ML workflow** from data ingestion to inference
- âœ… **Multiple models** trained and evaluated systematically
- âœ… **Comprehensive feature engineering** with temporal features and lag variables
- âœ… **Rigorous evaluation** with detailed metrics and visualizations
- âœ… **Model versioning** and artifact management
- âœ… **Inference pipeline** for production predictions

### Target Use Case

**Energy Consumption Forecasting** for:
- ğŸ¢ Smart grid optimization
- ğŸ’¡ Peak demand prediction
- ğŸ“Š Load balancing planning
- ğŸŒ Renewable energy integration
- ğŸ’° Cost optimization

---

## ğŸ¯ Key Challenges Addressed

| Challenge | Solution |
|-----------|----------|
| **Data Quality** | Outlier detection, missing value imputation, normalization |
| **Temporal Dependencies** | Rolling features, lag features, cyclical encoding |
| **Model Selection** | Multiple models trained, systematic comparison |
| **Overfitting** | Train/Validation/Test split, early stopping |
| **Production Deployment** | Model serialization, inference pipeline, scalability |
| **Reproducibility** | Fixed random state, configuration management |

---

## ğŸ—ï¸ Architecture

### Project Structure

```
energy-forecasting-pipeline/
â”œâ”€â”€ ğŸ“‚ data/
â”‚   â”œâ”€â”€ raw/                    # Original datasets
â”‚   â””â”€â”€ processed/              # Preprocessed data & artifacts
â”‚
â”œâ”€â”€ ğŸ“‚ src/                     # Core ML modules
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ config.py              # Configuration & hyperparameters
â”‚   â”œâ”€â”€ data_ingestion.py      # Data loading & exploration
â”‚   â”œâ”€â”€ preprocessing.py       # Cleaning & normalization
â”‚   â”œâ”€â”€ feature_engineering.py # Feature creation & selection
â”‚   â”œâ”€â”€ model_training.py      # Model training
â”‚   â”œâ”€â”€ evaluation.py          # Evaluation & visualization
â”‚   â””â”€â”€ utils.py               # Utility functions
â”‚
â”œâ”€â”€ ğŸ“‚ models/                  # Serialized models
â”‚   â”œâ”€â”€ best_model.pkl
â”‚   â”œâ”€â”€ scaler.pkl
â”‚   â””â”€â”€ feature_names.pkl
â”‚
â”œâ”€â”€ ğŸ“‚ scripts/
â”‚   â”œâ”€â”€ train_pipeline.py      # Main training orchestration
â”‚   â””â”€â”€ inference.py           # Prediction pipeline
â”‚
â”œâ”€â”€ ğŸ“‚ notebooks/
â”‚   â””â”€â”€ exploratory_analysis.ipynb  # EDA (optional)
â”‚
â”œâ”€â”€ ğŸ“‚ tests/                   # Unit tests
â”‚   â””â”€â”€ test_pipeline.py
â”‚
â”œâ”€â”€ requirements.txt            # Python dependencies
â”œâ”€â”€ setup.py                    # Package setup
â”œâ”€â”€ README.md                   # This file
â””â”€â”€ .gitignore
```

### Pipeline Flow

```
1. DATA INGESTION
   â†“
2. PREPROCESSING (Cleaning, Normalization, Outlier Detection)
   â†“
3. FEATURE ENGINEERING (Temporal Features, Rolling Stats, Lag Features)
   â†“
4. DATA SPLITTING (Train 70% / Validation 10% / Test 20%)
   â†“
5. MODEL TRAINING (Linear Regression, Random Forest, Gradient Boosting, XGBoost)
   â†“
6. MODEL EVALUATION (MAE, RMSE, RÂ², MAPE)
   â†“
7. MODEL SELECTION (Best model based on test RÂ²)
   â†“
8. ARTIFACT SAVING (Model, Scaler, Feature Names)
   â†“
9. INFERENCE (Predictions on new data)
```

---

## ğŸ“Š Dataset

### Energy Consumption Data
- **Source**: Synthetic + UCI ML Repository
- **Records**: ~8,700 hourly observations
- **Time Period**: 1 year of data
- **Features**: 19 environmental sensors (temperature, humidity)
- **Target**: Energy consumption (Appliances) in Wh

### Features Used
```
- Appliances (TARGET): Energy consumption in Wh
- lights: Lighting usage
- T1-T9: Temperature sensors (Celsius)
- RH_1-RH_9: Relative humidity sensors (%)
```

---

## ğŸ› ï¸ Models Implemented

| Model | Type | Hyperparameters | Best For |
|-------|------|-----------------|----------|
| **Linear Regression** | Parametric | - | Baseline, interpretability |
| **Random Forest** | Tree Ensemble | n_estimators=100, max_depth=15 | Feature importance, robustness |
| **Gradient Boosting** | Sequential Ensemble | n_estimators=100, lr=0.1 | Performance, complex patterns |
| **XGBoost** | Optimized Ensemble | n_estimators=100, lr=0.1 | Speed, scalability |

---

## ğŸ“ˆ Feature Engineering

### Temporal Features
- **Hour of day**: When electricity is consumed
- **Day of week**: Weekday vs weekend patterns
- **Month**: Seasonal effects
- **Cyclical encoding**: sin/cos transformation for periodic features

### Statistical Features
- **Rolling mean** (3h, 7h, 24h windows): Trend information
- **Rolling std** (3h, 7h, 24h windows): Volatility
- **Lag features** (1h, 2h, 3h, 24h): Historical dependencies

### Total Features: 40+

---

## ğŸ“Š Evaluation Metrics

```
MAE (Mean Absolute Error)    â†’ Average prediction error in absolute terms
RMSE (Root Mean Squared)     â†’ Penalizes large errors
RÂ² Score                      â†’ Proportion of variance explained
MAPE (Mean Absolute %)       â†’ Percentage error
```

### Typical Results
```
Best Model: Gradient Boosting
â”œâ”€â”€ Train RÂ²: 0.85-0.90
â”œâ”€â”€ Validation RÂ²: 0.78-0.85
â”œâ”€â”€ Test RÂ²: 0.75-0.82
â””â”€â”€ Test RMSE: 20-30 Wh
```

---

## ğŸš€ Quick Start

### Prerequisites
- Python 3.8+
- pip or conda

### Installation

1. **Clone the repository**
   ```bash
   git clone https://github.com/yourusername/energy-forecasting-pipeline.git
   cd energy-forecasting-pipeline
   ```

2. **Create virtual environment**
   ```bash
   python -m venv venv
   source venv/bin/activate  # On Windows: venv\Scripts\activate
   ```

3. **Install dependencies**
   ```bash
   pip install -r requirements.txt
   ```

### Training

**Run the complete pipeline**
```bash
python scripts/train_pipeline.py
```

This will:
- Load and explore data
- Preprocess and engineer features
- Train all models
- Evaluate and compare performance
- Save the best model and artifacts
- Generate visualizations

### Inference

**Make predictions on new data**
```bash
python scripts/inference.py
```

Or from Python:
```python
from scripts.inference import predict_energy_consumption
import pandas as pd

# Load your data
df_new = pd.read_csv("new_data.csv")

# Get predictions
predictions = predict_energy_consumption(df_new)
```

---

## ğŸ’» Code Quality

### Best Practices Implemented
- âœ… **Modular design**: Each step is an independent module
- âœ… **Configuration management**: Centralized `config.py`
- âœ… **Logging**: Comprehensive logging throughout pipeline
- âœ… **Error handling**: Graceful error management
- âœ… **Type hints**: Full type annotations for clarity
- âœ… **Documentation**: Docstrings for all functions
- âœ… **Reproducibility**: Fixed random seeds
- âœ… **Scalability**: Support for large datasets

### Code Structure Example
```python
# config.py: Central configuration
MODELS_CONFIG = {
    "Linear Regression": {...},
    "Random Forest": {...},
    "Gradient Boosting": {...},
    "XGBoost": {...}
}

# data_ingestion.py: Modular data loading
def load_data() -> pd.DataFrame:
    """Load and validate data"""
    
# preprocessing.py: Clear preprocessing pipeline
def prepare_data(df: pd.DataFrame) -> Tuple[pd.DataFrame, Dict]:
    """Handle missing values, outliers, normalization"""
    
# model_training.py: Systematic model training
def train_all_models(X_train, y_train) -> Dict:
    """Train multiple models in parallel"""
```

---

## ğŸ“Š Performance Analysis

### Model Comparison
The pipeline generates comprehensive evaluation reports including:
- Individual model metrics
- Train/Validation/Test performance
- Feature importance rankings
- Residual analysis
- Prediction error distributions

### Visualizations Generated
- âœ… Actual vs Predicted plots
- âœ… Residual analysis
- âœ… Feature importance charts
- âœ… Model performance comparison
- âœ… Error distribution histograms
- âœ… Q-Q plots for residuals

---

## ğŸ”§ Configuration

### Modifying Hyperparameters
Edit `src/config.py`:

```python
# Model parameters
MODELS_CONFIG = {
    "Random Forest": {
        "params": {
            "n_estimators": 150,      # Increase for better accuracy
            "max_depth": 20,          # Increase for complexity
            "random_state": RANDOM_STATE
        }
    }
}

# Data split
TEST_SIZE = 0.2                       # 20% test set
VALIDATION_SIZE = 0.1                # 10% validation set

# Features
ROLLING_WINDOW_SIZES = [3, 7, 24]   # Hour windows
```

---

## ğŸ§ª Testing

**Run tests**
```bash
python -m pytest tests/
```

**Manual testing**
```python
from src.preprocessing import prepare_data
from src.data_ingestion import load_data

df = load_data()
df_clean, metadata = prepare_data(df)
assert df_clean.shape[0] > 0
print("âœ“ Pipeline works correctly")
```

---

## ğŸ“ Artifacts Generated

After running `train_pipeline.py`, the following files are created:

```
models/
â”œâ”€â”€ best_model.pkl              # Serialized best model
â”œâ”€â”€ scaler.pkl                  # Fitted StandardScaler
â””â”€â”€ feature_names.pkl           # Expected feature names

data/processed/
â”œâ”€â”€ evaluation_report.csv       # Comprehensive metrics
â””â”€â”€ split_info.pkl              # Train/val/test splits

visualizations/
â”œâ”€â”€ *_predictions.png           # Actual vs Predicted
â”œâ”€â”€ *_feature_importance.png    # Top 20 features
â””â”€â”€ *_residuals.png             # Residual analysis
```

---

## ğŸ“ Learning Outcomes

This project demonstrates:

### Technical Skills
- ğŸ¯ End-to-end ML pipeline development
- ğŸ“Š Data preprocessing and feature engineering
- ğŸ¤– Model training and hyperparameter tuning
- ğŸ“ˆ Comprehensive evaluation and comparison
- ğŸ’¾ Model serialization and deployment

### Best Practices
- ğŸ“‹ Clean, modular code architecture
- ğŸ” Logging and monitoring
- ğŸ›¡ï¸ Error handling and validation
- ğŸ“š Documentation and type hints
- ğŸ”„ Reproducibility and versioning

### Production Concepts
- ğŸš€ Inference pipeline design
- ğŸ¯ Model evaluation for production
- ğŸ’¡ Feature management
- âš™ï¸ Configuration management
- ğŸ“Š Performance tracking

---

## ğŸ¤ Contributing

Contributions are welcome! Areas for improvement:
- [ ] Add cross-validation
- [ ] Implement hyperparameter optimization (Optuna, Ray Tune)
- [ ] Add deep learning models (LSTM for time-series)
- [ ] Implement MLflow for experiment tracking
- [ ] Add unit tests
- [ ] Create Docker containerization
- [ ] Add REST API endpoint
- [ ] Implement data validation with Great Expectations

---

## ğŸ“œ License

This project is licensed under the **MIT License** - see [LICENSE](LICENSE) for details.

---

## ğŸ‘¤ Author

**Youssef AIT ELOURF**
- ğŸ”— GitHub: [@youssefaitelourf](https://github.com/youssefaitelourf)
- ğŸ’¼ LinkedIn: [youssef-aitelourf](https://linkedin.com/in/youssef-aitelourf)
- ğŸ“§ Email: youssefaitelourf@gmail.com | youssef.aitelourf.pro@gmail.com

---

## ğŸ™ Acknowledgments

- Inspiration from production ML systems at leading tech companies
- Data sourced from UCI Machine Learning Repository
- Built with Python ML stack: scikit-learn, pandas, matplotlib

---

## ğŸ“– Additional Resources

### Recommended Reading
- [Scikit-learn Documentation](https://scikit-learn.org)
- [Feature Engineering Best Practices](https://www.kaggle.com/learn/feature-engineering)
- [ML Engineering Course by Andrew Ng](https://www.coursera.org/learn/machine-learning)

### Similar Projects to Explore
- [Kaggle Competitions](https://www.kaggle.com/competitions)
- [Fast.ai Course](https://www.fast.ai/)
- [MLOps.community](https://mlops.community/)

---

## â­ If this helps you, please consider giving it a star!

<div align="center">

**Made with â¤ï¸ for the ML community**

[â¬† Back to top](#-energy-forecasting-pipeline-advanced-ml-for-consumption-prediction)

</div>
